<!DOCTYPE html>
<html lang="en-us">
<!--  
====================================================
Homepage: http://localhost:4000
Credits: http://localhost:4000/disclosure
====================================================
-->
<head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<!-- link href="http://gmpg.org/xfn/11" rel="profile" -->
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8">

<!-- Enable responsiveness on mobile devices-->
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

<title>
  
    Generative Neural Machine Translation &middot; David Barber
  
</title>

<!-- Search Engine Optimization -->
<meta name="description" content="Natural Language Processing">
<meta name="keywords" content="deep learning, nlp, natural language processing, latent variable models, translation, neural machine translation, semi supervised learning">




<!-- Twitter Cards -->
  
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@davidobarber">
<meta name="twitter:image" content="http://web4.cs.ucl.ac.uk/staff/D.Barber/images/sitelogo.png">


<meta name="twitter:title" content="Generative Neural Machine Translation">
<meta name="twitter:description" content="Natural Language Processing">
<meta name="twitter:creator" content="@davidobarber">
<!-- End Twitter Cards -->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Generative Neural Machine Translation">
<meta property="og:description" content="Natural Language Processing">
<meta property="og:url" content="http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/">
<meta property="og:site_name" content="David Barber">

<meta property="og:image" content="http://web4.cs.ucl.ac.uk/staff/D.Barber/images/sitelogo.png">

<meta property="fb:app_id" content="1003108156422006">
<meta property="fb:admins" content="817465054">

<!-- Fonts -->
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400|Tangerine|Inconsolata">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/public/css/iconmoon.css">

<!-- CSS -->
<link rel="stylesheet" href="/public/css/style.min.css">

<!-- Add-on CSS to override system-wide defaults -->
<link rel="stylesheet" href="/public/css/addon.css">

<!-- CSS override per page -->


<!-- Java scripts -->
<!-- <script src="/public/js/jquery.min.js"></script> -->

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/icons/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/icons/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/icons/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/icons/apple-touch-icon-144x144-precomposed.png">
<!-- 180x180 (precomposed) for iPhone 6 -->
<link rel="apple-touch-icon-precomposed" sizes="180x180" href="http://localhost:4000/images/icons/apple-touch-icon-180x180.png">
<!-- 192x192 (precomposed) for Android -->
<link rel="icon" type="image/png" sizes="192x192"  href="http://localhost:4000/images/icons/android-icon-192x192.png">


<link rel="canonical" href="http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/">


<!-- RSS -->
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/feed.xml">


  <!--Load Mathjax-->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'" 
                }
            },
            showProcessingMessages: false
        });
    </script>

<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->



</head>


<!--<body class="theme-base-08">-->
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<!--<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">-->
<!--e<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" checked>-->

<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" >

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
<!--   <div class="sidebar-item">
    <p>David's blog...</p>
  </div> -->

  <nav class="sidebar-nav">
    <!-- a class="sidebar-nav-item" href="/">Home</a-->

    

    

    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/"><i class="iconside iconm-home"></i> Home</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/about/"><i class="iconside iconm-user"></i> About</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/blog/"><i class="iconside iconm-quill"></i> Blog</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/tags/"><i class="fa fa-tags"></i> Tags</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/archive/"><i class="fa fa-archive"></i> Archive</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/contact/"><i class="iconside iconm-envelop"></i> Contact</a>
                
    
            
                    <a class="sidebar-nav-item" href="https://twitter.com/davidobarber" target="_blank"><i class="iconside iconm-twitter"></i> Twitter</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/feed.xml"><i class="iconside iconm-feed2"></i> Feed</a>
                
         

    <a class="sidebar-nav-item" href=" http://www.cs.ucl.ac.uk/staff/d.barber/">UCL page</a>

  </nav>

<hr class="gh">




</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">David Barber</a>
            <!-- <small></small> -->
            <div class="headicons">
              <small><a href="http://localhost:4000/about" rel="me" title="About"><i class="iconm iconm-user"></i></a></small>
              <small><a href="http://localhost:4000/blog" rel="me" title="Blog"><i class="iconm iconm-quill"></i></a></small>
              <small><a href="http://localhost:4000/contact" rel="me" title="Contact"><i class="iconm iconm-envelop"></i></a></small> 
            </div>           
          </h3>
        </div>
      </div>

      <div class="container content">
        <!-- Look the author details up from the site config.

-->

<!-- Output author details if some exist.


<p>
  -->



Written by
    
        
Harshil Shah
        
    
        
            and David Barber
        
    

</p>    

<div class="post">
  <h1 itemprop="name" class="post-title">Generative Neural Machine Translation</h1>
  <span class="post-date" itemprop="datePublished" content="2018-09-12"><i class="fa fa-calendar"
  title="Date published"> <a class="permalink"
  href="http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/" itemprop="url" title="Permanent link to this post">12 Sep 2018</a> </i></span>
  
  <span class="post-tags" itemprop="keywords" content="deep learning, nlp, natural language processing, latent variable models, translation, neural machine translation, and semi supervised learning"><i class="fa fa-tags" title="page tags"></i> <a href="http://localhost:4000/tags/#deep+learning" title="Pages tagged deep learning" rel="tag">deep learning</a> &bull;  <a href="http://localhost:4000/tags/#nlp" title="Pages tagged nlp" rel="tag">nlp</a> &bull;  <a href="http://localhost:4000/tags/#natural+language+processing" title="Pages tagged natural language processing" rel="tag">natural language processing</a> &bull;  <a href="http://localhost:4000/tags/#latent+variable+models" title="Pages tagged latent variable models" rel="tag">latent variable models</a> &bull;  <a href="http://localhost:4000/tags/#translation" title="Pages tagged translation" rel="tag">translation</a> &bull;  <a href="http://localhost:4000/tags/#neural+machine+translation" title="Pages tagged neural machine translation" rel="tag">neural machine translation</a> &bull;  <a href="http://localhost:4000/tags/#semi+supervised+learning" title="Pages tagged semi supervised learning" rel="tag">semi supervised learning</a></span>
    
      <span class="social-icons"><a href="https://www.facebook.com/sharer.php?u=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/"  class="social-icons" target="_blank" title="Share on facebook"> <i class="fa fa-facebook meta"></i></a> 
<a href="https://twitter.com/share?text=Generative Neural Machine Translation&amp;url=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/&amp;via=davidobarber"  class="social-icons" target="_blank" title="Share on twitter"> <i class="fa fa-twitter meta"></i></a>
<a href="https://plus.google.com/share?url=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/"  class="social-icons" target="_blank" title="Share on Google+"> <i class="fa fa-google-plus"></i></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/"  class="social-icons" target="_blank" title="Share on LinkedIn"> <i class="fa fa-linkedin"></i></a>
<a href="http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/#disqus_thread" class="social-icons" title="Comments"><i class="fa fa-comments"></i></a>
<!--<a href="http://localhost:4000/featured" title="Featured posts"><i class="fa fa-paperclip" title="Featured" class="social-icons" style="vertical-align: top;"></i></a>-->
<a href="javascript:window.print()" class="social-icons" title="Printer friendly format"><i class="fa fa-print"></i></a>
</span>

    
  <!--Generative Neural Machine Translation-->

<h2 id="whats-wrong-with-current-machine-translation-models">What’s wrong with current Machine Translation models?</h2>

<!--more-->

<!--
* TOC
{:toc}
-->

<p>Machine Learning models are still largely superficial – the models don’t really ‘understand’ the meaning of the sentences they are translating. If we want increasingly ‘intelligent’ machines, it’s important that models begin to incorporate more knowledge of the world.  One approach to achieve this is to require models to be good, not only at translation, but additional tasks, such as question answering.  A parallel direction is to encourage models to internally focus on the meaning of the sentence. This post summarises our approach to this latter direction, published at NIPS 2018<sup id="fnref:SB2018"><a href="#fn:SB2018" class="footnote">1</a></sup>.</p>

<h2 class="no_toc" id="learning-meaningful-representations-of-data">Learning meaningful representations of data</h2>

<p>One possible method to learn meaningful representations of sentences is to use a latent variable model. Latent variable models are based on the hypothesis that for each sentence, there is a ‘hidden’ (or ‘latent’) vector which represents the sentence’s meaning, and that the sentence itself is a textual manifestation of that meaning.</p>

<p>Latent variable models in natural language processing typically posit the following generative process for sentences:</p>

<ul>
  <li>The ‘hidden’ or ‘latent’ representation of the sentence’s meaning is randomly generated according to a prior distribution.</li>
  <li>The sentence itself is then generated conditioned on this latent representation.</li>
</ul>

<p>Given a latent variable model and a sentence, the posterior distribution of the latent representation (i.e. the values of the representation that are likely to have generated that sentence) can be inferred. This posterior distribution can then be used for downstream tasks, e.g. the inferred representation could be used for answering questions about that sentence. Intuitively, the more information about the meaning of the sentence that the representation contains, the better it should be at performing at downstream tasks. Therefore, we would like to design a model which can use its latent representation to better represent the semantics of the text.</p>

<p>Most latent variable models use one latent representation per sentence in the data set. The problem with this approach is that there are no guarantees that the representations will learn semantically meaningful information about the text. For example, consider the two sentences: “she walked across the road” and “the woman crossed the street” - a basic latent variable model does not know a priori that walking across a road and crossing a street are similar actions. Therefore, the typical model would not be able to guarantee that the latent representations of these two sentences are similar.</p>

<p>Instead, if we were able to encode into the model that two sentences are semantically similar, we may be able to learn representations which better understand the meaning of the text. Unfortunately, large corpora of sentences with similar meanings in a single language are rare. However in the machine translation context, the same sentence expressed in different languages offers the potential to learn a latent variable model which better represents the sentence’s meaning. For example, a model which knows that the English sentence “the woman crossed the street” and the French sentence “la femme a traversé la rue” have the same meaning should be able to learn a representation with better semantic understanding.</p>

<h2 class="no_toc" id="generative-neural-machine-translation-gnmt">Generative Neural Machine Translation (GNMT)</h2>

<p>With Generative Neural Machine Translation (GNMT)<sup id="fnref:SB2018:1"><a href="#fn:SB2018" class="footnote">1</a></sup>, we use a single shared latent representation to model the same sentence in multiple languages. The latent variable is a language agnostic representation of the sentence; by giving it the responsiblity for modelling the same sentence in multiple languages, it is encouraged to learn the semantic meaning.</p>

<p>For each data point <script type="math/tex">n</script> in a data set, typical latent variable architectures model the joint distribution of the latent representation <script type="math/tex">\mathbf{z}^{(n)}</script> and the observed sentence <script type="math/tex">\mathbf{x}^{(n)}</script> as:</p>

<script type="math/tex; mode=display">p(\mathbf{z}^{(n)},\mathbf{x}^{(n)}) = p(\mathbf{z}^{(n)}) p_{\theta}(\mathbf{x}^{(n)}\vert \mathbf{z}^{(n)})</script>

<p>where <script type="math/tex">\theta</script> are trainable parameters of the model.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//fig_1.png" alt="SGVB" title="SGVB" /></p>

<p>Instead of modelling a single sentence per latent representation, GNMT uses a shared latent representation to model the same sentence both in the source and target languages. GNMT models the joint distribution of the latent representation, source sentence <script type="math/tex">\mathbf{x}^{(n)}</script> and target sentence <script type="math/tex">\mathbf{y}^{(n)}</script> as:</p>

<script type="math/tex; mode=display">p(\mathbf{z}^{(n)},\mathbf{x}^{(n)},\mathbf{y}^{(n)}) = p(\mathbf{z}^{(n)}) p_{\theta}(\mathbf{x}^{(n)}\vert \mathbf{z}^{(n)}) p_{\theta}(\mathbf{y}^{(n)}\vert \mathbf{z}^{(n)},\mathbf{x}^{(n)})</script>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//fig_2.png" alt="GNMT" title="GNMT" /></p>

<p>This set up means that <script type="math/tex">\mathbf{z}^{(n)}</script> models the commonality between the source and target sentences, which is the semantic meaning.</p>

<p>For full details on the neural networks used to model the distributions of the source <script type="math/tex">p_{\theta}(\mathbf{x}^{(n)}\vert\mathbf{z}^{(n)})</script> and target <script type="math/tex">p_{\theta}(\mathbf{y}^{(n)}\vert \mathbf{z}^{(n)},\mathbf{x}^{(n)})</script>, see <sup id="fnref:SB2018:2"><a href="#fn:SB2018" class="footnote">1</a></sup>.</p>

<p>One may argue that it would be better if the target sentence were dependent only on the latent representation and not directly on the source sentence, giving the model <script type="math/tex">p(\mathbf{x}\vert \mathbf{z})p(\mathbf{y}\vert \mathbf{z})p(\mathbf{z})</script> – forcing the latent representation to be fully responsible for generating both the source and target sentence. However, we found that the generated translations weren’t sufficiently syntactically coherent.</p>

<h2 id="training-the-model">Training the model</h2>

<p>We use the Stochastic Gradient Variational Bayes (SGVB)<sup id="fnref:KW2014"><a href="#fn:KW2014" class="footnote">2</a></sup><sup id="fnref:R2014"><a href="#fn:R2014" class="footnote">3</a></sup> algorithm to train the model described above. SGVB introduces a ‘recognition’ model <script type="math/tex">q_{\phi}(\mathbf{z}^{(n)}\vert \mathbf{x}^{(n)})</script> which acts as an approximation to the true but intractable posterior <script type="math/tex">p(\mathbf{z}^{(n)}\vert \mathbf{x}^{(n)})</script>, thus forming the following lower bound on the log likelihood of the observed data:</p>

<script type="math/tex; mode=display">\mathcal{L}(\mathbf{x}^{(n)}) = \mathbb{E}_{q_{\phi}(\mathbf{z}^{(n)}\vert \mathbf{x}^{(n)})} [\log p(\mathbf{z}^{(n)},\mathbf{x}^{(n)}) - \log q_{\phi}(\mathbf{z}^{(n)}\vert \mathbf{x}^{(n)})]</script>

<p>The model parameters <script type="math/tex">\theta</script> and recognition parameters <script type="math/tex">\phi</script> are then jointly learned by performing gradient ascent on this lower bound.</p>

<h2 id="generating-translations---the-banana-trick">Generating translations - the ‘banana trick’</h2>

<p>Suppose the model has been trained, and then we are given a sentence in the source language (<script type="math/tex">\mathbf{x}</script>) and asked to find a translation of that sentence. In this scenario, we want to find the most likely target sentence conditioned on the given source sentence. The natural objective is therefore:</p>

<script type="math/tex; mode=display">\mathbf{y}^{*} = \arg \max_{y} p(\mathbf{y}\vert \mathbf{x}) = \arg \max_{y} \int p_{\theta}(\mathbf{y}\vert \mathbf{z},\mathbf{x}) p(\mathbf{z}\vert \mathbf{x}) d\mathbf{z}</script>

<p>However this integral is intractable, and so we cannot perform this maximisation exactly. Instead, we perform approximate maximisation by iteratively refining a ‘guess’ for the target sentence. We first make a random guess for the target sentence, and then iterate between the following two steps:</p>

<ol>
  <li>Draw samples of the latent representation from the approximate posterior, using the source sentence and the latest guess for the target sentence.</li>
  <li>Update the guess for the target sentence based on the latent representation samples from step 1. This update is done by choosing <script type="math/tex">\mathbf{y}</script> to maximise <script type="math/tex">p_{\theta}(\mathbf{y}\vert \mathbf{z},\mathbf{x})</script>.</li>
</ol>

<p>Intuitively, this procedure computes the values of the latent representation that are likely to have generated both the source sentence and the latest guess for the target sentence. It then improves the guess based on those values of the latent variable. Mathematically, this iteratively increases a lower bound on <script type="math/tex">\log p(\mathbf{y}\vert \mathbf{x})</script> until convergence and is an application of the Expectation Maximisation algorithm, here <script type="math/tex">\mathbf{y}</script> playing the role of parameters.</p>

<p>Note: in our group we refer to this as the ‘banana trick’ because we are first aware of its usage in exercise 5.7 in <sup id="fnref:B2012"><a href="#fn:B2012" class="footnote">4</a></sup>, discussing a fictitious protein sequence in bananas.</p>

<p>Below, we show an example of a long sentence translated from English to French by GNMT. The long range coherence of the translation is a good indicator of the model’s ability to capture semantic information about the sentence within the latent representation.</p>

<p><strong>Source</strong>: Dans ce décret, il met en lumière les principales réalisations de la République d’Ouzbékistan dans le domaine de la protection et de la promotion des droits de l’homme et approuve le programme d’activités marquant le soixantième anniversaire de la déclaration universelle des droits de l’homme.</p>

<p><strong>Target</strong>: The decree highlights major achievements by the Republic of Uzbekistan in the field of protection and promotion of human rights and approves the programme of activities devoted to the sixtieth anniversary of the universal declaration of human rights.</p>

<p><strong>GNMT</strong>: In this decree, it highlights the main achievements of the Republic of Uzbekistan on the protection and promotion of human rights and approves the activities of the sixtieth anniversary of the universal declaration of human rights.</p>

<h2 id="dealing-with-missing-words">Dealing with missing words</h2>

<p>Because GNMT’s latent representation captures information about the meaning of the sentence rather than just the syntax, it is able to produce good translations even when there are missing words in the source sentence. The procedure for generating translations is similar to that described above –  however in this scenario we also have to refine a guess for the missing words in the source sentence.</p>

<p>We first make random guesses for the missing words in the source sentence and for the target sentence, and then iterate between the following three steps:</p>

<ol>
  <li>Draw samples of the latent representation from the approximate posterior, using the latest guesses for the source and target sentences.</li>
  <li>Update the guess for the source sentence based on the latent representation samples from step 1. This update is done by choosing <script type="math/tex">\mathbf{x}</script> to maximise <script type="math/tex">p_{\theta}(\mathbf{x}\vert \mathbf{z})</script>.</li>
  <li>Update the guess for the target sentence based on the latent representation samples from step 1 and on the updated guess for the source sentence from step 2. This update is done by choosing <script type="math/tex">\mathbf{y}</script> to maximise <script type="math/tex">p_{\theta}(\mathbf{y}\vert \mathbf{z},\mathbf{x})</script>.</li>
</ol>

<p>Below is an example of a sentence translated from Spanish to English, where the struck through words in the source sentence are considered missing. Using its latent representation, the model does remarkably well at imputing what the missing words may be and translating them accordingly.</p>

<p><strong>Source</strong>: Expresando su <del>satisfacción</del> por <del>la</del> asistencia que han <del>prestado</del> a los territorios no autónomos algunos <del>organismos</del> especializados y <del>otras</del> organizaciones del sistema de las naciones <del>unidas</del>, especialmente el <del>programa</del> de las naciones unidas <del>para</del> el desarrollo.</p>

<p><strong>Target</strong>: Welcoming the assistance extended to non-self-governing territories by certain specialized agencies and other organizations of the United Nations system, in particular the United Nations development programme.</p>

<p><strong>GNMT</strong>: Expressing its gratitude for the assistance given to non-self-governing territories by some specialized agencies and other organizations of the United Nations system, in particular from the development programmes of the United Nations.</p>

<h2 id="cross-language-parameter-sharing">Cross-language parameter sharing</h2>

<p>With the architecture described above, if we wanted to translate between, say, English (EN), Spanish (ES) and French (FR), we would have to train 6 separate models for EN → ES, ES → EN, EN → FR, etc. However because all three of these languages share somewhat similar structures, we may not lose much performance by sharing parameters. We therefore add two indicator variables to the model, one for the input language (<script type="math/tex">l_{x}</script>) and another for the output language (<script type="math/tex">l_{y}</script>). By doing this, we only have to train a single model to translate between three languages, instead of having 6 separate models.</p>

<p>The joint distribution of the latent representation, source sentence and target sentence becomes:</p>

<script type="math/tex; mode=display">p(\mathbf{z}^{(n)},\mathbf{x}^{(n)},\mathbf{y}^{(n)}\vert l_{x},l_{y}) = p(\mathbf{z}^{(n)}) p_{\theta}(\mathbf{x}^{(n)}\vert \mathbf{z}^{(n)},l_{x}) p_{\theta}(\mathbf{y}^{(n)}\vert \mathbf{z}^{(n)},\mathbf{x}^{(n)},l_{x},l_{y})</script>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//fig_3.png" alt="GNMT-Multi" title="GNMT-Multi" /></p>

<p>We refer to this version of the model as GNMT-Multi.</p>

<p>Overfitting is a phenomenon whereby a model is too closely fit to a particular set of data points. In machine translation, this often occurs when there aren’t enough paired sentences for the model to learn from. However, the cross language parameter sharing used for GNMT-Multi helps to mitigate this issue. This is because 6 separate models are essentially condensed into a single model, meaning that there aren’t enough parameters to allow the model to memorise the training data.</p>

<p>Below, we plot the BLEU scores comparing GNMT-Multi against 6 separate GNMT models, trained with only 400,000 pairs of translated sentences. BLEU is a measure of how well the generated translations match the true target sentences; higher is better. Clearly GNMT-Multi with a limited amount of available training data performs significantly better than each of the 6 separate GNMT models.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//gnmt_vs_gnmt_multi_1.png" alt="GNMT_vs_GNMT-Multi_1" title="GNMT vs. GNMT-Multi (trained with 400,000 sentence pairs)" /></p>

<p>When there is a large amount of training data available, we find that there is very little difference in the performance of GNMT and GNMT-Multi. Below, we plot the BLEU scores for the models trained with 4,000,000 sentence pairs; we find that there is no degradation in performance due to sharing parameters across languages!</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//gnmt_vs_gnmt_multi_2.png" alt="GNMT_vs_GNMT-Multi_2" title="GNMT vs. GNMT-Multi (trained with 4,000,000 sentence pairs)" /></p>

<h2 id="semi-supervised-learning">Semi-supervised learning</h2>

<p>Suppose, as in the previous section, that we only have access to a limited number of paired sentences, but that we now have available lots of untranslated sentences in each language. To learn from untranslated sentences, we can set the input language <script type="math/tex">l_{x}</script> and output language <script type="math/tex">l_{y}</script> to the same value, so that the model learns to reconstruct the sentence instead of translating it. Intuitively, this should further help the model to learn the structure and style of each language and thus produce more coherent translations at test time. We refer to this version of the model as GNMT-Multi-SSL.</p>

<p>Below we plot the BLEU scores of GNMT-Multi-SSL, GNMT-Multi and the 6 separate GNMT models. They are trained first with 400,000 then with 4,000,000 pairs of translated sentences. In both cases, they are also trained with approximately 20,900,000 untranslated English sentences, 2,700,000 untranslated Spanish sentences and 4,500,000 untranslated French sentences. GNMT-Multi-SSL clearly helps to mitigate overfitting when there are limited paired sentences available. In fact, GNMT-Multi-SSL trained with only 400,000 paired sentences performs about as well as each of the 6 separate GNMT models trained with 4,000,000 sentences! GNMT-Multi-SSL also produces higher BLEU scores even when there is lots of paired data; this verifies our intuition that adding monolingual data helps the model to develop a better understanding of each language individually, and output more coherent sentences accordingly.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//gnmt_vs_gnmt_multi_vs_gnmt_multi_ssl_1.png" alt="GNMT_vs_GNMT-Multi_vs_GNMT-Multi-SSL_1" title="GNMT vs. GNMT-Multi (trained with 400,000 sentence pairs)" /></p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//gnmt_vs_gnmt_multi_vs_gnmt_multi_ssl_2.png" alt="GNMT_vs_GNMT-Multi_vs_GNMT-Multi-SSL_2" title="GNMT vs. GNMT-Multi (trained with 4,000,000 sentence pairs)" /></p>

<h2 class="no_toc" id="summary">Summary</h2>

<p>We introduce Generative Neural Machine Translation (GNMT), which is a latent variable model that uses sentences with the same meaning in multiple languages to learn representations which better understand the semantics of the text. It can be used to translate a source sentence by iteratively refining a guess for the target sentence and updating the latent representation accordingly. Because it captures the meaning of the sentence, GNMT is particularly effective at producing translations when there are missing words in the source sentence. We also introduce GNMT-Multi, which is a single unified model (instead of one per language pair) to mitigate overfitting when there is limited paired data available. Finally, we leverage large amounts of untranslated sentences to help the model to further learn the structure and style of each language and produce more coherent translations.</p>

<h3 class="no_toc" id="references">References</h3>

<div class="footnotes">
  <ol>
    <li id="fn:SB2018">
      <p>H. Shah and D. Barber. Generative Neural Machine Translation. In Advances in Neural Information Processing Systems, 2018. <a href="#fnref:SB2018" class="reversefootnote">&#8617;</a> <a href="#fnref:SB2018:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:SB2018:2" class="reversefootnote">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:KW2014">
      <p>D. Kingma and M. Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations, 2014. <a href="#fnref:KW2014" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:R2014">
      <p>D. Rezende et al. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In Proceedings of the 31st International Conference on Machine Learning, PMLR 32, pages 1278–1286, 2014. <a href="#fnref:R2014" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:B2012">
      <p>D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press, 2016. <a href="#fnref:B2012" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  <span class="social-icons"><a href="https://www.facebook.com/sharer.php?u=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/"  class="social-icons" target="_blank" title="Share on facebook"> <i class="fa fa-facebook meta"></i></a> 
<a href="https://twitter.com/share?text=Generative Neural Machine Translation&amp;url=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/&amp;via=davidobarber"  class="social-icons" target="_blank" title="Share on twitter"> <i class="fa fa-twitter meta"></i></a>
<a href="https://plus.google.com/share?url=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/"  class="social-icons" target="_blank" title="Share on Google+"> <i class="fa fa-google-plus"></i></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/"  class="social-icons" target="_blank" title="Share on LinkedIn"> <i class="fa fa-linkedin"></i></a>
<a href="http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/#disqus_thread" class="social-icons" title="Comments"><i class="fa fa-comments"></i></a>
<!--<a href="http://localhost:4000/featured" title="Featured posts"><i class="fa fa-paperclip" title="Featured" class="social-icons" style="vertical-align: top;"></i></a>-->
<a href="javascript:window.print()" class="social-icons" title="Printer friendly format"><i class="fa fa-print"></i></a>
</span>

<!--

<div class="share-page">
    Share this on &rarr;
    <a href="https://facebook.com/sharer.php?u=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a>
</div>
-->
  <hr>
  
  <span class="post-date metafoot" itemprop="datePublished" content="2018-09-12"><i class="fa fa-calendar" title="Date published"> <a class="permalink" href="http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/" itemprop="url" title="Permanent link to this post">12 Sep 2018</a> </i></span>
  <span class="post-tags" itemprop="keywords" content="deep learning, nlp, natural language processing, latent variable models, translation, neural machine translation, and semi supervised learning"><i class="fa fa-tags" title="page tags"></i> <a href="http://localhost:4000/tags/#deep+learning" title="Pages tagged deep learning" rel="tag">deep learning</a> &bull;  <a href="http://localhost:4000/tags/#nlp" title="Pages tagged nlp" rel="tag">nlp</a> &bull;  <a href="http://localhost:4000/tags/#natural+language+processing" title="Pages tagged natural language processing" rel="tag">natural language processing</a> &bull;  <a href="http://localhost:4000/tags/#latent+variable+models" title="Pages tagged latent variable models" rel="tag">latent variable models</a> &bull;  <a href="http://localhost:4000/tags/#translation" title="Pages tagged translation" rel="tag">translation</a> &bull;  <a href="http://localhost:4000/tags/#neural+machine+translation" title="Pages tagged neural machine translation" rel="tag">neural machine translation</a> &bull;  <a href="http://localhost:4000/tags/#semi+supervised+learning" title="Pages tagged semi supervised learning" rel="tag">semi supervised learning</a></span>
    
</div>


  <div class="printMsg">
<table>
  <thead>
    <tr>
      <th><i class="fa fa-twitter">@davidobarber</i></th>
      <th>QR code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><i class="fa fa-anchor"> http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/</i><br /><i class="fa fa-calendar"> 12-Sep-18</i><br /><i class="fa fa-creative-commons"> BY-NC-SA 4.0 http://localhost:4000/disclosure</i></td>
      <td><img src="https://chart.googleapis.com/chart?chs=150x150&cht=qr&chl=http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/&choe=UTF-8" alt="http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/" /></td>
    </tr>
  </tbody>
</table>
</div>



<div class="page-break"></div>
<div class="related">
  <h2>Related Posts</h2>
<ul>
  
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/11/07/Learning-From-Scratch-by-Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/">Learning From Scratch by Thinking Fast and Slow with Deep Learning and Tree Search</a><br /></li>
          
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/">Some modest insights into the error surface of Neural Nets</a><br /></li>
          
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/04/03/variational-optimisation/">Evolutionary Optimization as a Variational Method</a><br /></li>
          
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/03/15/large-number-of-classes/">Training with a large number of classes</a><br /></li>
          
    
  
</ul>
</div>

<div class="prevnext">
  
    <a class="prevnext-item older" href="http://localhost:4000/blog/2017/11/07/Learning-From-Scratch-by-Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/" title="Learning From Scratch by Thinking Fast and Slow with Deep Learning and Tree Search">Older</a>
  
  
    <span class="prevnext-item older">Newer</span>
  
</div>

<div class="page-break"></div>

<div id="disqus_thread"></div><!-- /#disqus_thread -->



                    <div class="custom-footer" style="display: block;">
            <div class="footer-social-icons">
            <ul class="social-icons">
                      
            </ul>
            </div>
            </div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
    

<!-- gist embed -->

  <!--Gist embed -->
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.4/gist-embed.min.js"></script>



<!-- disqus comments -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'davidbarber'; 
    var disqus_identifier = 'http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


</body>
</html>
