<!DOCTYPE html>
<html lang="en-us">
<!--  
====================================================
Homepage: http://localhost:4000
Credits: http://localhost:4000/disclosure
====================================================
-->
<head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<!-- link href="http://gmpg.org/xfn/11" rel="profile" -->
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8">

<!-- Enable responsiveness on mobile devices-->
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

<title>
  
    Some modest insights into the error surface of Neural Nets &middot; David Barber
  
</title>

<!-- Search Engine Optimization -->
<meta name="description" content="Local Optima in Deep Learning">
<meta name="keywords" content="deep learning, optimisation, Gauss Newton, local optima">




<!-- Twitter Cards -->
  
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@davidobarber">
<meta name="twitter:image" content="http://web4.cs.ucl.ac.uk/staff/D.Barber/images/sitelogo.png">


<meta name="twitter:title" content="Some modest insights into the error surface of Neural Nets">
<meta name="twitter:description" content="Local Optima in Deep Learning">
<meta name="twitter:creator" content="@davidobarber">
<!-- End Twitter Cards -->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Some modest insights into the error surface of Neural Nets">
<meta property="og:description" content="Local Optima in Deep Learning">
<meta property="og:url" content="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/">
<meta property="og:site_name" content="David Barber">

<meta property="og:image" content="http://web4.cs.ucl.ac.uk/staff/D.Barber/images/sitelogo.png">

<meta property="fb:app_id" content="1003108156422006">
<meta property="fb:admins" content="817465054">

<!-- Fonts -->
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400|Tangerine|Inconsolata">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/public/css/iconmoon.css">

<!-- CSS -->
<link rel="stylesheet" href="/public/css/style.min.css">

<!-- Add-on CSS to override system-wide defaults -->
<link rel="stylesheet" href="/public/css/addon.css">

<!-- CSS override per page -->


<!-- Java scripts -->
<!-- <script src="/public/js/jquery.min.js"></script> -->

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/icons/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/icons/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/icons/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/icons/apple-touch-icon-144x144-precomposed.png">
<!-- 180x180 (precomposed) for iPhone 6 -->
<link rel="apple-touch-icon-precomposed" sizes="180x180" href="http://localhost:4000/images/icons/apple-touch-icon-180x180.png">
<!-- 192x192 (precomposed) for Android -->
<link rel="icon" type="image/png" sizes="192x192"  href="http://localhost:4000/images/icons/android-icon-192x192.png">


<link rel="canonical" href="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/">


<!-- RSS -->
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/feed.xml">


  <!--Load Mathjax-->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'" 
                }
            },
            showProcessingMessages: false
        });
    </script>

<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->



</head>


<!--<body class="theme-base-08">-->
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<!--<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">-->
<!--e<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" checked>-->

<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" >

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
<!--   <div class="sidebar-item">
    <p>David's blog...</p>
  </div> -->

  <nav class="sidebar-nav">
    <!-- a class="sidebar-nav-item" href="/">Home</a-->

    

    

    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/"><i class="iconside iconm-home"></i> Home</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/about/"><i class="iconside iconm-user"></i> About</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/blog/"><i class="iconside iconm-quill"></i> Blog</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/tags/"><i class="fa fa-tags"></i> Tags</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/archive/"><i class="fa fa-archive"></i> Archive</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/contact/"><i class="iconside iconm-envelop"></i> Contact</a>
                
    
            
                    <a class="sidebar-nav-item" href="https://twitter.com/davidobarber" target="_blank"><i class="iconside iconm-twitter"></i> Twitter</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/feed.xml"><i class="iconside iconm-feed2"></i> Feed</a>
                
         

    <a class="sidebar-nav-item" href=" http://www.cs.ucl.ac.uk/staff/d.barber/">UCL page</a>

  </nav>

<hr class="gh">




</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">David Barber</a>
            <!-- <small></small> -->
            <div class="headicons">
              <small><a href="http://localhost:4000/about" rel="me" title="About"><i class="iconm iconm-user"></i></a></small>
              <small><a href="http://localhost:4000/blog" rel="me" title="Blog"><i class="iconm iconm-quill"></i></a></small>
              <small><a href="http://localhost:4000/contact" rel="me" title="Contact"><i class="iconm iconm-envelop"></i></a></small> 
            </div>           
          </h3>
        </div>
      </div>

      <div class="container content">
        <!-- Look the author details up from the site config.

-->

<!-- Output author details if some exist.


<p>
  -->




</p>    

<div class="post">
  <h1 itemprop="name" class="post-title">Some modest insights into the error surface of Neural Nets</h1>
  <span class="post-date" itemprop="datePublished" content="2017-07-30"><i class="fa fa-calendar"
  title="Date published"> <a class="permalink"
  href="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/" itemprop="url" title="Permanent link to this post">30 Jul 2017</a> </i></span>
  
  <span class="post-tags" itemprop="keywords" content="deep learning, optimisation, Gauss Newton, and local optima"><i class="fa fa-tags" title="page tags"></i> <a href="http://localhost:4000/tags/#deep+learning" title="Pages tagged deep learning" rel="tag">deep learning</a> &bull;  <a href="http://localhost:4000/tags/#optimisation" title="Pages tagged optimisation" rel="tag">optimisation</a> &bull;  <a href="http://localhost:4000/tags/#Gauss+Newton" title="Pages tagged Gauss Newton" rel="tag">Gauss Newton</a> &bull;  <a href="http://localhost:4000/tags/#local+optima" title="Pages tagged local optima" rel="tag">local optima</a></span>
    
      <span class="social-icons"><a href="https://www.facebook.com/sharer.php?u=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/"  class="social-icons" target="_blank" title="Share on facebook"> <i class="fa fa-facebook meta"></i></a> 
<a href="https://twitter.com/share?text=Some modest insights into the error surface of Neural Nets&amp;url=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/&amp;via=davidobarber"  class="social-icons" target="_blank" title="Share on twitter"> <i class="fa fa-twitter meta"></i></a>
<a href="https://plus.google.com/share?url=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/"  class="social-icons" target="_blank" title="Share on Google+"> <i class="fa fa-google-plus"></i></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/"  class="social-icons" target="_blank" title="Share on LinkedIn"> <i class="fa fa-linkedin"></i></a>
<a href="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/#disqus_thread" class="social-icons" title="Comments"><i class="fa fa-comments"></i></a>
<!--<a href="http://localhost:4000/featured" title="Featured posts"><i class="fa fa-paperclip" title="Featured" class="social-icons" style="vertical-align: top;"></i></a>-->
<a href="javascript:window.print()" class="social-icons" title="Printer friendly format"><i class="fa fa-print"></i></a>
</span>

    
  <p>Did you know that feedforward Neural Nets (with piecewise linear transfer functions) have no smooth local maxima?</p>

<p>In our recent ICML paper <a href="http://proceedings.mlr.press/v70/botev17a.html">Practical Gauss-Newton Optimisation for Deep Learning</a>) we discuss a second order method that can be applied successfully to accelerate training of Neural Networks. However, here I want to discuss some of the fairly straightforward, but perhaps interesting, insights into the geometry of the error surface that that work gives.</p>

<!--more-->


<p><script type="math/tex">\newcommand{\br}[1]{\left(#1\right)}</script>
<script type="math/tex">\newcommand{\sq}[1]{\left[#1\right]}</script>
<script type="math/tex">\newcommand{\ave}[1]{\mathbb{E}\sq{#1}}</script></p>

<h2 class="no_toc" id="feedforward-neural-networks">Feedforward Neural Networks</h2>

<p>In our description, a feedforward NN takes an input vector <script type="math/tex">x</script> and produces a vector <script type="math/tex">h_L</script> on the final <script type="math/tex">L^{th}</script> layer. We write <script type="math/tex">h_\lambda</script> to be the vector of pre-activation values for layer <script type="math/tex">\lambda</script> and <script type="math/tex">a_\lambda</script> to denote the vector of activation values after passing through the transfer function <script type="math/tex">f_\lambda</script>.</p>

<p>Starting with setting <script type="math/tex">a_0</script>  to the input <script type="math/tex">x</script>, a feedforward NN is defined by the recursion</p>

<script type="math/tex; mode=display">h_\lambda = W_\lambda a_{\lambda-1}</script>

<p>where <script type="math/tex">W_\lambda</script> is the weight matrix of layer <script type="math/tex">\lambda</script> (we use a sub or superscript <script type="math/tex">\lambda</script> wherever most convenient) and the activation vector is given by</p>

<script type="math/tex; mode=display">a_\lambda = f_\lambda(h_\lambda)</script>

<p>We define a loss <script type="math/tex">E(h_L,y)</script> between the final output layer <script type="math/tex">h_L</script> and a desired training output <script type="math/tex">y</script>. For example, we might use a squared loss</p>

<script type="math/tex; mode=display">E(h_L,y) = (h_L-y)^2</script>

<p>where the loss is summed over all elements of the vector.  For a training dataset the total error function is the summed  loss over individual training points</p>

<script type="math/tex; mode=display">\bar{E}(\theta) = \sum_{n=1}^N E(h_L(n),y(n))</script>

<p>where <script type="math/tex">\theta</script> represents the stacked vector of all parameters of the network. For simplicity we will write <script type="math/tex">E(\theta)</script> for the error for a single generic datapoint.</p>

<h3 class="no_toc" id="the-gradient">The Gradient</h3>

<p>For training a NN, a key quantity is the gradient of the error</p>

<script type="math/tex; mode=display">g_{i} = \frac{\partial}{\partial\theta_i} E(\theta)</script>

<p>We use this for example in gradient descent training algorithms. An important issue is how to compute the gradient efficiently. Thanks to the layered structure of the network, it’s intuitive that there is an efficient scheme (backprop which is a special case of Reverse Mode AutoDiff) that propagates information from layer to layer.</p>

<h3 class="no_toc" id="the-hessian">The Hessian</h3>

<p>One aspect of the structure of the error surface is the local curvature, defined by the Hessian matrix with elements</p>

<script type="math/tex; mode=display">H_{ij} = \frac{\partial^2}{\partial\theta_i\partial\theta_j} E(\theta)</script>

<p>The Hessian matrix itself is typically very large. To make this more manageable, we’ll focus here on the Hessian of the parameters of a given layer <script type="math/tex">\lambda</script>.  That is</p>

<script type="math/tex; mode=display">[H_\lambda]_{(a,b),(c,d)} = \frac{\partial^2 E}{\partial W^\lambda_{a,b}\partial W^\lambda_{c,d}}</script>

<p>The Hessians <script type="math/tex">H_\lambda</script> then form the diagonal block matrices of the full Hessian <script type="math/tex">H</script>.</p>

<h2 class="no_toc" id="a-recursion-for-the-hessian">A recursion for the Hessian</h2>

<p>Similar to the gradient, it’s perhaps intuitive that a recursion exists to calculate this layerwise Hessian.  Starting from</p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial W^\lambda_{a,b}}=\sum_i \frac{\partial h^\lambda_i}{W^\lambda_{a,b}}\frac{\partial E}{\partial h^\lambda_i} = a^{\lambda-1}_b\frac{\partial E}{\partial h^\lambda_a}</script>

<p>and differentiating again we obtain</p>

<script type="math/tex; mode=display">[H_\lambda]_{(a,b),(c,d)} = a^{\lambda-1}_b a^{\lambda-1}_d [{\cal{H}}_\lambda]_{a,c}
\tag{1}\label{eq:H}</script>

<p>where we define the pre-activation Hessian for layer <script type="math/tex">\lambda</script> as</p>

<script type="math/tex; mode=display">[{\cal{H}}_\lambda]_{a,c} = \frac{\partial^2 E}{\partial h^\lambda_a\partial h^\lambda_c}</script>

<p>We show in <a href="http://proceedings.mlr.press/v70/botev17a.html">Practical Gauss-Newton Optimisation for Deep Learning</a> that one can derive a simple backwards recursion for this pre-activation Hessian (the recursion is for a single datapoint – the total Hessian <script type="math/tex">\bar{H}</script> is a sum over the individual datapoint Hessians):</p>

<script type="math/tex; mode=display">{\cal{H}}_\lambda = B_\lambda W_{\lambda+1}^\top {\cal{H}}_{\lambda+1}W_{\lambda+1}B_{\lambda}+D_\lambda
\tag{2}\label{eq:recursion}</script>

<p>where we define the diagonal matrices</p>

<script type="math/tex; mode=display">B_\lambda = \text{diag}(f'_\lambda(h_\lambda))</script>

<p>and</p>

<script type="math/tex; mode=display">D_\lambda = \text{diag}\br{f''_\lambda(h_\lambda)\frac{\partial E}{\partial a_\lambda}}</script>

<p>Here <script type="math/tex">f'</script> is the first derivative of the transfer function and <script type="math/tex">f''</script> is the second derivative.</p>

<p>The recursion is initialised with <script type="math/tex">{\cal{H}}_L</script> which depends on the objective <script type="math/tex">E(h_L,y)</script> and is easily calculated for the usual loss functions. For example, for the square loss <script type="math/tex">(y-h_L)^2/2</script> we have <script type="math/tex">{\cal{H}}_L=I</script>, namely the identity matrix. We use this recursion in our <a href="http://proceedings.mlr.press/v70/botev17a.html">paper</a> to build an approximate Gauss-Newton optimisation method.</p>

<h2 class="no_toc" id="consequences">Consequences</h2>

<p>Piecewise linear transfer functions, such as the ReLU <script type="math/tex">f(x) = \max(x,0)</script> are currently popular due to both their speed of evaluation (compared to more traditional transfer functions such as <script type="math/tex">\tanh(x)</script>) and also the empirical observation that, under gradient based training, they tend to get trapped less often in local optima. Note that if the transfer functions are piecewise linear, this does not necessarily mean that the objective will be piecewise linear (since the loss is usually itself not piecewise linear).</p>

<p>For a piecewise linear transfer function, apart from the `nodes’ where the linear sections meet, the function is differentiable and has zero second derivative, <script type="math/tex">f''(x)=0</script>. This means that the matrices <script type="math/tex">D_\lambda</script> in the above Hessian recursion will be zero (away from nodes).</p>

<p>For many common loss functions, such as squared loss (for regression) and cross entropy loss (for classification) the Hessian <script type="math/tex">{\cal{H}}_L</script> is Positive Semi-Definite (PSD).</p>

<p>Note that, according to \eqref{eq:recursion}, for transfer functions that contain zero gradient points <script type="math/tex">f'(x)=0</script> then the Hessian <script type="math/tex">H_\lambda</script> can have lower rank than <script type="math/tex">H_{\lambda+1}</script>, reducing the curvature information propagating back from layers close to the output towards layers closer to the input. This has the effect of creating flat plateaus in the surface and makes gradient based training potentially more problematic. Conversely, provided the gradient of the transfer function is never zero <script type="math/tex">f'\neq 0</script>, then according to \eqref{eq:recursion} each layer pre-activation Hessian is Positive Definite, helping preserve the propagation of surface curvature back through the network.</p>

<h3 class="no_toc" id="structure-within-a-layer">Structure within a layer</h3>

<p>For such loss functions, it follows that the pre-activation Hessian <script type="math/tex">{\cal{H}}_\lambda</script> for all layers is PSD as well (away from nodes).  It immediately follows from \eqref{eq:H} that the Hessian <script type="math/tex">H_\lambda</script> for each layer <script type="math/tex">\lambda</script> is PSD.  This means that, if we fix all the parameters of the network, and vary only  the parameters in a layer <script type="math/tex">W^\lambda</script>, then the objective <script type="math/tex">E</script> can exhibit no smooth local maxima or smooth saddle points.  Note that this does not imply that the objective is convex everywhere with respect to <script type="math/tex">W_\lambda</script> as the surface will contain ridges corresponding to the non-differentiable nodes.</p>

<h3 class="no_toc" id="no-differentiable-local-maxima">No differentiable local maxima</h3>

<p>The trace of the full Hessian <script type="math/tex">H</script> is the sum of the traces of each of the layerwise blocks <script type="math/tex">H_\lambda</script>. Since (as usual away from nodes) by the above argument each matrix <script type="math/tex">H_\lambda</script> is PSD, it follows that the trace of the full Hessian is non-negative.  This means that it is not possible for all eigenvalues of the Hessian to be simultaneously negative, with the immediate consequence that feedforward networks (with piecewise linear transfer functions) have no differentiable local maxima. The picture below illustrates the kind of situtation therefore that can happen in terms of local maxima (test):</p>

<!--![blogpost_canhappen](http://web4.cs.ucl.ac.uk/staff/D.Barber/images//blogpost_canhappen.png)-->
<p class="text-center"><img src="http://localhost:4000/images/blogpost_canhappen.png" alt="blogpost_canhappen" /></p>

<p>whereas the image below depicts the kind of smooth local maxima that cannot happen:</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//blogpost_canthappen.png" alt="blogpost_canthappen" title="cannot happen" /></p>

<h3 class="no_toc" id="visualisation-for-a-simple-two-layer-net">Visualisation for a simple two layer net</h3>

<p>We consider a simple network with two layers, ReLU transfer functions and square loss error. The network thus has two weight matrices <script type="math/tex">W^1</script> and <script type="math/tex">W^2</script>.  Below we choose two fixed matrices <script type="math/tex">U</script> and <script type="math/tex">V</script> and parameterise the weight matrix <script type="math/tex">W^1</script> as a function of two scalars <script type="math/tex">u</script> and <script type="math/tex">v</script>, so that <script type="math/tex">W^1(u,v)=uU + vV</script>.  As we vary <script type="math/tex">u</script> and <script type="math/tex">v</script> we then plot the objective function <script type="math/tex">E(u,v)</script>, keeping all other parameters of the network fixed.</p>

<p>As we can see the surface contains no local differentiable local maxima as we vary the parameters in the layer.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images/rectlinE1.png" alt="rectlinE1" title="rectlin E1" /></p>

<p>Below we show an analogous plot for varying the parameters of the second layer weights <script type="math/tex">W^2(u,v)</script>, which has the same predicted property that there are no differentiable local maxima.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//rectlinE2.png" alt="rectlinE2" title="rectlin E2" /></p>

<p>Finally, below we plot <script type="math/tex">E(u,v)</script> using <script type="math/tex">W^1=uU</script> and <script type="math/tex">W^2=vV</script>, showing how the objective function changes as we simultaneously change the parameters in different layers. As we can see, there are no differentiable maxima.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//rectlinE12.png" alt="rectlinE12" title="rectlin E12" /></p>

<h1 class="no_toc" id="summary">Summary</h1>

<p>A simple consequence of using piecewise linear transfer functions and a convex loss, is that feedforward networks cannot have any differentiable maxima (or saddle points) as parameters are varied within a layer. Furthermore, the objective cannot contain any differentiable maxima, even as we vary parameters across layers. Note that the objective <script type="math/tex">E(u,v)</script> though can (and empirically does) have smooth saddle points as one varies parameters <script type="math/tex">u</script> and <script type="math/tex">v</script> across <em>different</em> layers.</p>

<p>It’s unclear how practically significant these modest insights are. However, they do potentially partially support the use of piecewise linear transfer functions (particularly those with no zero gradient regions) since for such transfer functions  gradient based training algorithms cannot easily dawdle on local maxima (anywhere), or idle around saddle points (within a layer) since such regions correspond to sharp slopes in the objective.</p>

<p>These results are part of a more detailed study of second order methods for optimisation in feedforward Neural Nets which will appear in <a href="http://proceedings.mlr.press/v70/botev17a.html">ICML 2017</a>.</p>

<!--
{:.no_toc}
-->

  <span class="social-icons"><a href="https://www.facebook.com/sharer.php?u=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/"  class="social-icons" target="_blank" title="Share on facebook"> <i class="fa fa-facebook meta"></i></a> 
<a href="https://twitter.com/share?text=Some modest insights into the error surface of Neural Nets&amp;url=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/&amp;via=davidobarber"  class="social-icons" target="_blank" title="Share on twitter"> <i class="fa fa-twitter meta"></i></a>
<a href="https://plus.google.com/share?url=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/"  class="social-icons" target="_blank" title="Share on Google+"> <i class="fa fa-google-plus"></i></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/"  class="social-icons" target="_blank" title="Share on LinkedIn"> <i class="fa fa-linkedin"></i></a>
<a href="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/#disqus_thread" class="social-icons" title="Comments"><i class="fa fa-comments"></i></a>
<!--<a href="http://localhost:4000/featured" title="Featured posts"><i class="fa fa-paperclip" title="Featured" class="social-icons" style="vertical-align: top;"></i></a>-->
<a href="javascript:window.print()" class="social-icons" title="Printer friendly format"><i class="fa fa-print"></i></a>
</span>

<!--

<div class="share-page">
    Share this on &rarr;
    <a href="https://facebook.com/sharer.php?u=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a>
</div>
-->
  <hr>
  
  <span class="post-date metafoot" itemprop="datePublished" content="2017-07-30"><i class="fa fa-calendar" title="Date published"> <a class="permalink" href="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/" itemprop="url" title="Permanent link to this post">30 Jul 2017</a> </i></span>
  <span class="post-tags" itemprop="keywords" content="deep learning, optimisation, Gauss Newton, and local optima"><i class="fa fa-tags" title="page tags"></i> <a href="http://localhost:4000/tags/#deep+learning" title="Pages tagged deep learning" rel="tag">deep learning</a> &bull;  <a href="http://localhost:4000/tags/#optimisation" title="Pages tagged optimisation" rel="tag">optimisation</a> &bull;  <a href="http://localhost:4000/tags/#Gauss+Newton" title="Pages tagged Gauss Newton" rel="tag">Gauss Newton</a> &bull;  <a href="http://localhost:4000/tags/#local+optima" title="Pages tagged local optima" rel="tag">local optima</a></span>
    
</div>


  <div class="printMsg">
<table>
  <thead>
    <tr>
      <th><i class="fa fa-twitter">@davidobarber</i></th>
      <th>QR code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><i class="fa fa-anchor"> http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/</i><br /><i class="fa fa-calendar"> 30-Jul-17</i><br /><i class="fa fa-creative-commons"> BY-NC-SA 4.0 http://localhost:4000/disclosure</i></td>
      <td><img src="https://chart.googleapis.com/chart?chs=150x150&cht=qr&chl=http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/&choe=UTF-8" alt="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/" /></td>
    </tr>
  </tbody>
</table>
</div>



<div class="page-break"></div>
<div class="related">
  <h2>Related Posts</h2>
<ul>
  
     
       
        
          <li><a href="http://localhost:4000/blog/2018/09/12/AI-as-a-coworker/">AI Coworking</a><br /></li>
          
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2018/09/12/Generative-Neural-Machine-Translation/">Generative Neural Machine Translation</a><br /></li>
          
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/11/07/Learning-From-Scratch-by-Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/">Learning From Scratch by Thinking Fast and Slow with Deep Learning and Tree Search</a><br /></li>
          
    
  
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/04/03/variational-optimisation/">Evolutionary Optimization as a Variational Method</a><br /></li>
          
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/03/15/large-number-of-classes/">Training with a large number of classes</a><br /></li>
          
    
  
</ul>
</div>

<div class="prevnext">
  
    <a class="prevnext-item older" href="http://localhost:4000/blog/2017/04/03/variational-optimisation/" title="Evolutionary Optimization as a Variational Method">Older</a>
  
  
    <a class="prevnext-item older" href="http://localhost:4000/blog/2017/11/07/Learning-From-Scratch-by-Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/" title="Learning From Scratch by Thinking Fast and Slow with Deep Learning and Tree Search">Newer</a>
  
</div>

<div class="page-break"></div>

<div id="disqus_thread"></div><!-- /#disqus_thread -->



                    <div class="custom-footer" style="display: block;">
            <div class="footer-social-icons">
            <ul class="social-icons">
                      
            </ul>
            </div>
            </div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
    

<!-- gist embed -->

  <!--Gist embed -->
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.4/gist-embed.min.js"></script>



<!-- disqus comments -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'davidbarber'; 
    var disqus_identifier = 'http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


</body>
</html>
